{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RITaDGIkUv01"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_retinal_image(image):\n",
        "    # Randomly flipping the image horizontally\n",
        "    if np.random.rand() < 0.5:\n",
        "        image = cv2.flip(image, 1)\n",
        "        \n",
        "    # Randomly rotating the image by a small angle\n",
        "    angle = np.random.randint(-5, 5)\n",
        "    M = cv2.getRotationMatrix2D((image.shape[1]/2, image.shape[0]/2), angle, 1)\n",
        "    image = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]))\n",
        "    \n",
        "    # Randomly applying brightness and contrast changes\n",
        "    alpha = np.random.uniform(0.5, 1.5)\n",
        "    image = cv2.convertScaleAbs(image, alpha=alpha, beta=np.random.randint(-50, 50))\n",
        "\n",
        "    # Randomly zooming the image\n",
        "    zoom = np.random.uniform(0.9, 1.1)\n",
        "    M = cv2.getRotationMatrix2D((image.shape[1]/2, image.shape[0]/2), 0, zoom)\n",
        "    image = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]))\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "id": "m7gsI8P5VTEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_segmentation_dataset(data_dir, augment_fn=None):\n",
        "    # Create dataset of all image paths\n",
        "    train_image_paths = tf.io.gfile.glob(f\"{data_dir}/train/*.jpg\")\n",
        "    train_images_ds = tf.data.Dataset.from_tensor_slices(train_image_paths)\n",
        "\n",
        "    test_image_paths = tf.io.gfile.glob(f\"{data_dir}/test/*.jpg\")\n",
        "    test_images_ds = tf.data.Dataset.from_tensor_slices(test_image_paths)\n",
        "\n",
        "    # Read and decode the images\n",
        "    def read_and_decode(path):\n",
        "        image = tf.io.read_file(path)\n",
        "        image = tf.image.decode_jpeg(image, channels=3)\n",
        "        if augment_fn:\n",
        "            image = augment_fn(image)\n",
        "        return image\n",
        "    train_images_ds = train_images_ds.map(read_and_decode, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    test_images_ds = test_images_ds.map(read_and_decode, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    # Read and decode the corresponding label images\n",
        "    def read_and_decode_masks(path):\n",
        "        mask = tf.io.read_file(path.replace(\"jpg\", \"png\"))\n",
        "        mask = tf.image.decode_png(mask, channels=1)\n",
        "        if augment_fn:\n",
        "            mask = augment_fn(mask)\n",
        "        return mask\n",
        "    train_masks_ds = train_images_ds.map(read_and_decode_masks)\n",
        "    test_masks_ds = test_images_ds.map(read_and_decode_masks)\n",
        "\n",
        "    # Zip the images and labels together to get a dataset of (image, label) pairs\n",
        "    train_ds = tf.data.Dataset.zip((train_images_ds, train_masks_ds))\n",
        "    test_ds = tf.data.Dataset.zip((test_images_ds, test_masks_ds))\n",
        "\n",
        "    return train_ds, test_ds\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4NopGYBXVTCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use this function to create the train and test datasets as follows:\n",
        "\n",
        "\n",
        "\n",
        "This function takes two arguments, the path of the data directory and an optional function for data augmentation. It creates a dataset of all image paths in the train folder and test folder, it then uses the tf.data.Dataset.map() method to read and decode the images, applying the optional data augmentation function, if provided. It also reads and decode the corresponding label images, which are in PNG format. Finally, it zips the images and labels\n"
      ],
      "metadata": {
        "id": "fNbmJzVhVhoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augment_fn = lambda x: augment_retinal_image(x)\n",
        "train_ds, test_ds = create_segmentation_dataset(\"path/to/data/dir\", augment_fn)"
      ],
      "metadata": {
        "id": "e2Q-dY0uVdsa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}